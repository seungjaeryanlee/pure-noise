{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5797e364-5737-4976-90e9-65af251ba11c",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea828912-7305-4f16-b31d-8dfc1f154bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "cifar10_images = [(255 * image.numpy()).astype(np.uint8) for image, _ in train_dataset]\n",
    "# np.in1d trick - https://stackoverflow.com/a/16216866\n",
    "cifar10_images_bytes = np.array([cifar10_image.tobytes() for cifar10_image in cifar10_images])\n",
    "cifar10_labels = [label for _, label in train_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d07e94-7a25-44f0-b5c9-2b71ac4835b5",
   "metadata": {},
   "source": [
    "## Cui et al. (ClassBalancedLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2903e8-4b63-464d-b10d-5e41fe1a146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 00:52:44.782203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-30 00:52:45.548169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-30 00:52:45.548243: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-30 00:52:45.548253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-30 00:52:46.459194: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-30 00:52:47.949131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21446 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:00:08.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filenames = [\"tfrecords/data/cifar-10-data-im-0.01/train.tfrecords\"]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847ed34b-4c57-4a92-bbe4-26008fe75397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12406, 3, 32, 32), (12406,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = [], []\n",
    "for raw_record in raw_dataset:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    image_bytes = example.features.feature['image'].bytes_list.value\n",
    "    image_tf = tf.io.decode_raw(image_bytes, tf.uint8)\n",
    "    image = tf.reshape(image_tf, [1, 3, 32, 32]).numpy()\n",
    "    images.extend(image)\n",
    "    label = example.features.feature['label'].int64_list.value\n",
    "    labels.extend(label)\n",
    "\n",
    "cui_images = np.array(images)\n",
    "cui_labels = np.array(labels)\n",
    "cui_images.shape, cui_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c85b070-3e5a-49f0-bcf9-495d70d4b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_images_bytes = np.array([cui_image.tobytes() for cui_image in cui_images])\n",
    "cui_intersection = np.in1d(cifar10_images_bytes, cui_images_bytes)\n",
    "cui_indices = np.where(cui_intersection)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df67ef0-c2be-4965-8f02-837ec9d97ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cui_indices.txt\", \"w\") as f:\n",
    "    for i in cui_indices:\n",
    "        f.write(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0dba49-4408-49b2-9a23-773f94a6517b",
   "metadata": {},
   "source": [
    "## M2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9185302-5f96-43fe-a815-08831674f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d45724-f63d-477e-84d4-9ab9a795564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599484250318941\n",
      "(5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50)\n"
     ]
    }
   ],
   "source": [
    "from m2m_data_loader import make_longtailed_imb, get_imbalanced\n",
    "\n",
    "N_CLASSES = 10\n",
    "N_SAMPLES = 5000\n",
    "N_SAMPLES_PER_CLASS_BASE = [int(N_SAMPLES)] * N_CLASSES\n",
    "N_SAMPLES_PER_CLASS_BASE = make_longtailed_imb(N_SAMPLES, N_CLASSES, 100)\n",
    "N_SAMPLES_PER_CLASS_BASE = tuple(N_SAMPLES_PER_CLASS_BASE)\n",
    "print(N_SAMPLES_PER_CLASS_BASE)\n",
    "\n",
    "train_default_loader, valid_loader, test_loader = get_imbalanced(train_dataset, valid_dataset, N_SAMPLES_PER_CLASS_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9363cb42-d5bd-4ec4-bbd6-dbd2011f047f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12406, 3, 32, 32), (12406,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "m2m_images = []\n",
    "m2m_labels = []\n",
    "for images, labels in train_default_loader:\n",
    "    m2m_images.extend(images.numpy())\n",
    "    m2m_labels.extend(labels.numpy())\n",
    "m2m_images = np.array(m2m_images)\n",
    "m2m_labels = np.array(m2m_labels)\n",
    "m2m_images.shape, m2m_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8974aff3-7a8e-4ae0-a4a5-cd1c2dfb29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m_images = (255*m2m_images).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb220c2-223c-48d3-a46d-ae2dd34a3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m_images_bytes = np.array([m2m_image.tobytes() for m2m_image in m2m_images])\n",
    "m2m_intersection = np.in1d(cifar10_images_bytes, m2m_images_bytes)\n",
    "m2m_indices = np.where(m2m_intersection)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd5068-6e79-41fb-a0a6-941aa07e122c",
   "metadata": {},
   "source": [
    "## LDAM-DRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0c0837-98ac-4687-8fbd-075aff0b1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n",
    "    cls_num = 10\n",
    "\n",
    "    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n",
    "        np.random.seed(rand_number)\n",
    "        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n",
    "        self.gen_imbalanced_data(img_num_list)\n",
    "\n",
    "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
    "        img_max = len(self.data) / cls_num\n",
    "        img_num_per_cls = []\n",
    "        if imb_type == 'exp':\n",
    "            for cls_idx in range(cls_num):\n",
    "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "                img_num_per_cls.append(int(num))\n",
    "        elif imb_type == 'step':\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max))\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max * imb_factor))\n",
    "        else:\n",
    "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "        return img_num_per_cls\n",
    "\n",
    "    def gen_imbalanced_data(self, img_num_per_cls):\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        targets_np = np.array(self.targets, dtype=np.int64)\n",
    "        classes = np.unique(targets_np)\n",
    "        # np.random.shuffle(classes)\n",
    "        self.num_per_cls_dict = dict()\n",
    "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "            self.num_per_cls_dict[the_class] = the_img_num\n",
    "            idx = np.where(targets_np == the_class)[0]\n",
    "            np.random.shuffle(idx)\n",
    "            selec_idx = idx[:the_img_num]\n",
    "            new_data.append(self.data[selec_idx, ...])\n",
    "            new_targets.extend([the_class, ] * the_img_num)\n",
    "        new_data = np.vstack(new_data)\n",
    "        self.data = new_data\n",
    "        self.targets = new_targets\n",
    "        \n",
    "    def get_cls_num_list(self):\n",
    "        cls_num_list = []\n",
    "        for i in range(self.cls_num):\n",
    "            cls_num_list.append(self.num_per_cls_dict[i])\n",
    "        return cls_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e9d9ff2-4a3a-4e83-bdcf-d4326e2a4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ldam_train_dataset = IMBALANCECIFAR10(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f09cb464-eb3e-4100-abd3-bdbdef2ca274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ldam_images = [(255 * image.numpy()).astype(np.uint8) for image, _ in ldam_train_dataset]\n",
    "ldam_labels = [label for _, label in ldam_train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9df1277-dc69-40c2-85ab-1371093c070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.in1d trick - https://stackoverflow.com/a/16216866\n",
    "ldam_images_bytes = np.array([ldam_image.tobytes() for ldam_image in ldam_images])\n",
    "ldam_intersection = np.in1d(cifar10_images_bytes, ldam_images_bytes)\n",
    "ldam_indices = np.where(ldam_intersection)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dcb94a5-da02-4008-8556-8bd6f513010c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12406"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ldam_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ab675-f41d-4aa0-bf30-d0eec9d2b292",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88c143d1-ec46-47a1-bbbc-1c045fec5101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4610"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ldam_indices) - set(m2m_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8358686f-8351-4cd9-a702-ba8a37aa46b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4615"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(cui_indices) - set(m2m_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b484122-96f1-43cd-8fc6-2c23de2b2633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4548"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ldam_indices) - set(cui_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
