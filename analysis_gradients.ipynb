{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a29bd97-39e9-4656-8d34-3dd8522c66a7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1438b3c-af14-4967-8c1f-8789edca58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea211d-edd3-40aa-bed1-de6e2f9c6c75",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d562ff18-0a25-4503-8d1d-1f31bdc5a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG = OmegaConf.create({\n",
    "    # Dataset\n",
    "    \"dataset\": \"CIFAR-10-LT\",\n",
    "    \"ir_ratio\": 100,\n",
    "    \"num_classes\": 10,\n",
    "    \"train_transform_reprs\": [\n",
    "        \"RandomHorizontalFlip()\",\n",
    "        \"RandomCrop(32, padding=4)\",\n",
    "        \"ToTensor()\",\n",
    "        \"Cutout(n_holes=1, length=16)\",\n",
    "        \"RandomApply([ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8)\",\n",
    "        \"RandomGrayscale(p=0.2)\",\n",
    "        \"RandomApply([GaussianBlur(kernel_size=3, sigma=[.1, 2.])], p=0.5)\",\n",
    "    ],\n",
    "    \"valid_transform_reprs\": [\"ToTensor()\"],\n",
    "\n",
    "    # Mean and std for normalization\n",
    "    \"normalize_mean\": [0.4914, 0.4822, 0.4465],\n",
    "    \"normalize_std\": [0.2023, 0.1994, 0.2010],\n",
    "\n",
    "    # DataLoader\n",
    "    \"batch_size\": 128,\n",
    "    \"num_workers\": 8,\n",
    "    \"enable_pin_memory\": True,\n",
    "\n",
    "    # Model\n",
    "    \"model\": \"ResNet-32-akamaster\",\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \n",
    "\n",
    "    # OPeNz\n",
    "    \"pure_noise_image_size\": 32,\n",
    "    \"delta\": 0.333333,\n",
    "    \"pure_noise_mean\": [0.4914, 0.4822, 0.4465],\n",
    "    \"pure_noise_std\": [0.2023, 0.1994, 0.2010],\n",
    "    \"pure_noise_image_size\": 32,\n",
    "\n",
    "    # BN\n",
    "    \"noise_bn_option\": \"DARBN\",\n",
    "\n",
    "    # Checkpoint\n",
    "    \"checkpoint_filename\": \"ResNet__epoch_199.pt\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cc54f7-df5c-4f5d-b711-67f03176f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da632875-6ac2-4dc9-854e-47692e327103",
   "metadata": {},
   "source": [
    "## Download checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6424292-740d-45ed-a33a-ea55d8b3ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "from storage import CHECKPOINT_URLS\n",
    "\n",
    "checkpoint_filepath = f\"checkpoints/{CONFIG.checkpoint_filename}\"\n",
    "if not os.path.exists(checkpoint_filepath):\n",
    "    gdown.download(CHECKPOINT_URLS[CONFIG.checkpoint_filename], checkpoint_filepath, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4553f6-7098-44ad-ada1-592c3340a9cc",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd50050-6595-43f0-9072-9909d508ebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f34facc5550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f26e30-3733-4ee3-bfa7-ea0ae0927bb8",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73cdc970-f948-4da7-99ac-6a3843e973c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initializers import initialize_model, InputNormalize\n",
    "from models.noise_bn_option import NoiseBnOption\n",
    "\n",
    "net = initialize_model(\n",
    "    model_name=CONFIG.model, \n",
    "    num_classes=CONFIG.num_classes, \n",
    "    noise_bn_option=NoiseBnOption[CONFIG.noise_bn_option],\n",
    "    dropout_rate=CONFIG.dropout_rate)\n",
    "net = net.to(device)\n",
    "\n",
    "normalizer = InputNormalize(\n",
    "    torch.Tensor(CONFIG.normalize_mean).to(device), \n",
    "    torch.Tensor(CONFIG.normalize_std).to(device)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0d7c92-0088-4ac3-b59d-0ee6f9f19854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkpointing import load_checkpoint\n",
    "\n",
    "load_checkpoint(net, optimizer=None, checkpoint_filepath=checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c41431a-84a9-47ee-974a-edce0842a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2ad5e-9901-40cd-86a7-6602e82bb2bc",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa54ddb0-e01e-44f5-b05b-f4b4169f258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from datasets.imbalanced_cifar import IMBALANCECIFAR10, IMBALANCECIFAR100\n",
    "from datasets.sampling import count_class_frequency, compute_class_weights_on_effective_num_samples, compute_sample_weights\n",
    "from models.noise_bn_option import NoiseBnOption\n",
    "\n",
    "from datasets.cifar10lt import build_train_dataset, build_valid_dataset\n",
    "from initializers import initialize_transforms\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "NUM_CLASSES = CONFIG.num_classes\n",
    "train_transform = initialize_transforms(CONFIG.train_transform_reprs)\n",
    "train_dataset = IMBALANCECIFAR10(root=DATA_ROOT, train=True, transform=train_transform, download=True, ir_ratio=CONFIG.ir_ratio)\n",
    "\n",
    "class_frequency = count_class_frequency(train_dataset.targets, NUM_CLASSES)\n",
    "class_weights = 1. / class_frequency\n",
    "sample_weights = compute_sample_weights(train_dataset.targets, class_weights)\n",
    "num_samples = len(train_dataset)\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=num_samples, # https://stackoverflow.com/a/67802529\n",
    "    replacement=True,\n",
    ")\n",
    "train_oversampling_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    shuffle=False,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    num_workers=CONFIG.num_workers,\n",
    "    pin_memory=CONFIG.enable_pin_memory,\n",
    ")\n",
    "train_loader = train_oversampling_loader\n",
    "\n",
    "valid_transform = initialize_transforms(CONFIG.valid_transform_reprs)\n",
    "valid_dataset = CIFAR10(root=DATA_ROOT, train=False, transform=valid_transform, download=True)\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    num_workers=CONFIG.num_workers,\n",
    "    pin_memory=CONFIG.enable_pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67655df1-38e2-4323-949e-e302718c5764",
   "metadata": {},
   "source": [
    "## Setup Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0209e21f-5706-47da-8c70-3a89153b7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ef779-447c-40ce-9d9f-773ec5cc38c0",
   "metadata": {},
   "source": [
    "## Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23dfb3b6-ba42-4b16-979f-32e02839510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(net):\n",
    "    grads = torch.cat([torch.flatten(param.grad) for param in net.parameters()]).cpu()\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1892121f-0342-4bd0-9783-be548e73a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup train dataset with correct transforms\n",
    "num_samples_per_class = torch.Tensor(class_frequency).to(device)\n",
    "pure_noise_mean = torch.Tensor(CONFIG.pure_noise_mean).to(device)\n",
    "pure_noise_std = torch.Tensor(CONFIG.pure_noise_std).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c0469b-95c8-4889-bff5-d5cef83c0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from replace_with_pure_noise import replace_with_pure_noise\n",
    "\n",
    "ENABLE_OPEN = False\n",
    "\n",
    "net.train()\n",
    "train_labels = []\n",
    "train_gradients = []\n",
    "train_batch_size = []\n",
    "for minibatch_i, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs = inputs.float().to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    inputs = normalizer(inputs)\n",
    "\n",
    "    net.zero_grad()\n",
    "    noise_mask = replace_with_pure_noise(\n",
    "        images=inputs,\n",
    "        targets=labels,\n",
    "        delta=CONFIG.delta,\n",
    "        num_samples_per_class=num_samples_per_class,\n",
    "        dataset_mean=pure_noise_mean,\n",
    "        dataset_std=pure_noise_std,\n",
    "        image_size=CONFIG.pure_noise_image_size,\n",
    "    ) if ENABLE_OPEN else None\n",
    "    outputs = net(inputs, noise_mask=noise_mask)\n",
    "    losses = criterion(outputs, labels)\n",
    "    losses.backward()\n",
    "\n",
    "    gradients = get_gradients(net)\n",
    "\n",
    "    train_labels.extend(labels.cpu().detach().tolist())\n",
    "    train_gradients.append(gradients.cpu().detach().tolist())\n",
    "    train_batch_size.append(len(labels))\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "train_gradients = np.array(train_gradients)\n",
    "train_batch_size = np.array(train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c062ff-9a73-43e0-8a62-e358337e35de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 464154)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1875401-f273-4106-9e04-b67d8c34dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean gradient magnitude: 4.990390937790069\n"
     ]
    }
   ],
   "source": [
    "train_magnitudes = np.linalg.norm(train_gradients, axis=1, ord=2)\n",
    "mean_gradient_magnitude = np.sum(train_magnitudes * train_batch_size) / len(train_dataset)\n",
    "print(f\"Mean gradient magnitude: {mean_gradient_magnitude}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863c5604-e9b6-4f2a-a60d-dd29ea95cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_gradient(train_gradients, train_batch_size):\n",
    "    weights = np.full(train_gradients.shape, np.expand_dims(train_batch_size, axis=1))\n",
    "    mean_gradient = np.sum(train_gradients * weights, axis=0) / sum(train_batch_size)\n",
    "\n",
    "    return mean_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c5a2208-9d44-4023-8e61-eb028ab70f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/13849249\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1022f477-5d3e-48a9-92ea-5934c88ab18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = []\n",
    "mean_gradient = compute_mean_gradient(train_gradients, train_batch_size)\n",
    "for train_gradient in train_gradients:\n",
    "    angle = angle_between(train_gradient, mean_gradient)\n",
    "    angles.append(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dae5576-4c54-4d47-9bcc-93765c06ef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directional variance: 0.0017780182707009143\n"
     ]
    }
   ],
   "source": [
    "directional_variance = np.var(angles)\n",
    "print(f\"Directional variance: {directional_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb785233-c70d-42ec-aa2e-4e733500ddb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
