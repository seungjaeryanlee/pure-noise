{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db3a3c8-f8a6-46f8-9a97-22fd99ba0baa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917ed30-62af-43a3-bf14-87272091f85b",
   "metadata": {},
   "source": [
    "Description from Section D.2:\n",
    "\n",
    "![Description from Section D.2](images/paper__image_augmentations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e49e797-f7cf-4f9d-a40d-e6cae0fe0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Hyperparameters\n",
    "DATASET__IMAGE_HEIGHT = 32\n",
    "DATASET__IMAGE_WIDTH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9734eff9-24c0-46f4-8909-81036431a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import custom_transforms\n",
    "\n",
    "# transforms.ToTensor() not needed as we use torchvision.io.read_image,\n",
    "# which gives torch.Tensor instead of PIL.Image\n",
    "# Data Augmentation transforms are mostly from Bazinga699/NCL\n",
    "# https://github.com/Bazinga699/NCL/blob/2bbf193/lib/dataset/cui_cifar.py#L64\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    custom_transforms.Cutout(n_holes=1, length=16),\n",
    "    # TODO: Check if this is correct values for SIMCLR augmentation\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=[.1, 2.])\n",
    "    ], p=0.5),\n",
    "    # TODO: Verify where this number comes from: is it CIFAR-10 or CIFAR-10-LT?\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57e2e98-eec3-4aba-8efb-1fd5d4728552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cifar10 import CIFAR10LTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a7bbfa-b4be-4a08-973c-9794c9e20226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_filepath = \"data/json/cifar10_imbalance100/cifar10_imbalance100_train.json\"\n",
    "train_images_dirpath = \"data/json/cifar10_imbalance100/images/\"\n",
    "train_dataset = CIFAR10LTDataset(\n",
    "    json_filepath=train_json_filepath,\n",
    "    images_dirpath=train_images_dirpath,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac486f42-2b29-4d45-a598-385042e0e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_json_filepath = \"data/json/cifar10_imbalance100/cifar10_imbalance100_valid.json\"\n",
    "valid_images_dirpath = \"data/json/cifar10_imbalance100/images/\"\n",
    "valid_dataset = CIFAR10LTDataset(\n",
    "    json_filepath=valid_json_filepath,\n",
    "    images_dirpath=valid_images_dirpath,\n",
    "    transform=valid_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf8745f-6316-4f38-84c1-18581caeb2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12406, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8acab3-8de7-4930-b879-9b2208fa3596",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139825f5-9b45-4cc7-804e-d864ce2c1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader Hyperparameters\n",
    "DATALOADER__NUM_WORKERS = 4\n",
    "DATALOADER__BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210e0e53-2c2a-40c2-bbd0-886616b0df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "labels = np.arange(10)\n",
    "with open(train_json_filepath, \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "sample_labels = [annotation[\"category_id\"] for annotation in json_data[\"annotations\"]]\n",
    "sample_labels_count = torch.LongTensor([len(np.where(sample_labels == l)[0]) for l in labels])\n",
    "weights = 1. / sample_labels_count\n",
    "sample_weights = torch.FloatTensor([weights[l] for l in sample_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bccb704e-4d0e-4ac9-94e3-9db48d9a8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=50000, # https://stackoverflow.com/a/67802529\n",
    "    replacement=True,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=DATALOADER__BATCH_SIZE,\n",
    "    num_workers=DATALOADER__NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19d3c2b-d8ef-4fc5-9c08-74d9babe7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=DATALOADER__BATCH_SIZE,\n",
    "    num_workers=DATALOADER__NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41564a9-afd2-496c-b96e-0b4f9040db9b",
   "metadata": {},
   "source": [
    "## OPeN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664ff4e-0cf6-432d-af31-5ef7dfd694c8",
   "metadata": {},
   "source": [
    "<img src=\"images/paper__noise_image.png\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdab6034-5539-4f8d-b4d2-cda9172f9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPeN Hyperparameters\n",
    "OPEN__NOISE_RATIO = 1/3\n",
    "OPEN__START_EPOCH = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53f11a22-b358-4cff-89a5-ac454dfec7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all training images\n",
    "train_images = []\n",
    "for minibatch_images, _ in train_loader:\n",
    "    train_images.extend(minibatch_images)\n",
    "train_images = torch.stack(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e1fa813-54dc-4eaa-9d4c-fe7962c930ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7539, -0.7452, -0.6490]) tensor([1.4275, 1.4369, 1.4134])\n"
     ]
    }
   ],
   "source": [
    "# Find mean and std per channel\n",
    "mean_per_channel = train_images.mean(dim=(0, 2, 3))\n",
    "std_per_channel = train_images.std(dim=(0, 2, 3))\n",
    "print(mean_per_channel, std_per_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aff3664-f60d-4bf3-80e2-8f31393b469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edited code from paper to include clipping\n",
    "def sample_noise_images(mean_per_channel, std_per_channel, n_images, height, width):\n",
    "    \"\"\"Samples pure noise images from the normal distribution N(mean,std)\"\"\"\n",
    "    r = torch.normal(mean_per_channel[0], std_per_channel[0], size=(n_images, 1, height, width))\n",
    "    g = torch.normal(mean_per_channel[1], std_per_channel[1], size=(n_images, 1, height, width))\n",
    "    b = torch.normal(mean_per_channel[2], std_per_channel[2], size=(n_images, 1, height, width))\n",
    "    pure_noise_images = torch.cat((r, g, b), 1)\n",
    "    clipped_pure_noise_images = torch.clip(pure_noise_images, min=0, max=1)\n",
    "\n",
    "    return clipped_pure_noise_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd187c-b45a-49b7-91dd-951720f1bb01",
   "metadata": {},
   "source": [
    "<img src=\"images/paper__open.png\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "addf12cc-85fa-44a9-a6f8-ba97f1e8ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def replace_images_with_pure_noise(\n",
    "    images,\n",
    "    labels,\n",
    "    mean_per_channel,\n",
    "    std_per_channel,\n",
    "    height,\n",
    "    width,\n",
    "    noise_ratio,\n",
    "):\n",
    "    # Compute representation ratio\n",
    "    # TODO: Should `sample_labels_count` be a parameter to this method?\n",
    "    representation_ratio = sample_labels_count[labels] / torch.max(sample_labels_count)\n",
    "\n",
    "    # Compute probabilities to replace natural images with pure noise images\n",
    "    noise_probs = (1 - representation_ratio) * noise_ratio\n",
    "\n",
    "    # Sample indexes to replace with noise according to Bernoulli distribution\n",
    "    noise_indices = torch.nonzero(torch.bernoulli(noise_probs)).view(-1)\n",
    "\n",
    "    # Replace natural images with sampled pure noise images\n",
    "    noise_images = sample_noise_images(\n",
    "        mean_per_channel,\n",
    "        std_per_channel,\n",
    "        n_images=len(noise_indices),\n",
    "        height=height,\n",
    "        width=width,\n",
    "    )\n",
    "    images[noise_indices] = noise_images\n",
    "\n",
    "    # Create mask for noise images - later used by DAR-BN\n",
    "    noise_mask = torch.zeros(images.size(0), dtype=torch.bool)\n",
    "    noise_mask[noise_indices] = True\n",
    "\n",
    "    return images, noise_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c0688-22cf-4900-acc3-c54608332a57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09745284-43dc-47f5-9ef9-9bfd479de060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "MODEL__WIDERESNET_DEPTH = 28\n",
    "MODEL__WIDERESNET_K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e331334-268a-4fc5-8d6a-21e45b0896f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import WideResNet\n",
    "\n",
    "net = WideResNet(\n",
    "    num_classes=10,\n",
    "    depth=MODEL__WIDERESNET_DEPTH,\n",
    "    widen_factor=MODEL__WIDERESNET_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33260cab-02b3-4850-8c4a-e35e80f8e4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36479194"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b4f9a70-ae5a-410d-8640-aa673e981ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cbd19-4d6b-442c-ad4f-d731160cdc41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8025d5c2-6781-42f0-bfb5-5254a9414b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseungjaeryanlee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a69ed-fd35-4c03-9519-0095f4315891",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a90c9d5f-e0f1-4765-8c3b-f6d16f23d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Hyperparameters\n",
    "OPTIM__LR = 0.1\n",
    "OPTIM__MOMENTUM = 0.9\n",
    "OPTIM__WEIGHT_DECAY = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e56120d-408a-432d-9eff-ddfd0e37af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=OPTIM__LR,\n",
    "    momentum=OPTIM__MOMENTUM,\n",
    "    weight_decay=OPTIM__WEIGHT_DECAY,\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1,\n",
    "    gamma=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fbe8e-b9cd-4287-bcbf-2d40b3081acb",
   "metadata": {},
   "source": [
    "## Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "208cf7e0-df2f-45a3-849c-49cc542d4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "N_EPOCH = 5\n",
    "SAVE_CKPT_EVERY_N_EPOCH = 10\n",
    "LOAD_CKPT = False\n",
    "LOAD_CKPT_FILEPATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "451ccd42-20a2-4679-9ef3-9e3f26e72a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3282946-999f-4ff3-8786-2b17f84eeb31",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "046cc698-7fef-486e-b705-da632b963f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    checkpoint_filepath: str,\n",
    "):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    checkpoint_filepath: str,\n",
    "):\n",
    "    checkpoint = torch.load(checkpoint_filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bf0b5f1-8d8f-45ff-bc2e-7feb04b9aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CKPT:\n",
    "    load_checkpoint(net, optimizer, LOAD_CKPT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51ab08a5-1503-4c02-bf5c-4104a8d720dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseungjaeryanlee\u001b[0m (\u001b[33mbrianryan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/pure-noise/wandb/run-20221226_150146-1tk5q7b2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/brianryan/pure-noise/runs/1tk5q7b2\" target=\"_blank\">woven-plant-48</a></strong> to <a href=\"https://wandb.ai/brianryan/pure-noise\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"pure-noise\",\n",
    "    entity=\"brianryan\",\n",
    ")\n",
    "\n",
    "wandb.config.update({\n",
    "    # Dataset\n",
    "    \"dataset__image_height\": DATASET__IMAGE_HEIGHT,\n",
    "    \"dataset__image_width\": DATASET__IMAGE_WIDTH,\n",
    "    # DataLoader\n",
    "    \"dataloader__num_workers\": DATALOADER__NUM_WORKERS,\n",
    "    \"dataloader__batch_size\": DATALOADER__BATCH_SIZE,\n",
    "    # OPeN\n",
    "    \"open__start_epoch\": OPEN__START_EPOCH,\n",
    "    \"open__noise_ratio\": OPEN__NOISE_RATIO,\n",
    "    # Optimizer\n",
    "    \"optim__lr\": OPTIM__LR,\n",
    "    \"optim__momentum\": OPTIM__MOMENTUM,\n",
    "    \"optim__weight_decay\": OPTIM__WEIGHT_DECAY,\n",
    "    # Model\n",
    "    \"model__wideresnet_depth\": MODEL__WIDERESNET_DEPTH,\n",
    "    \"model__wideresnet_k\": MODEL__WIDERESNET_K,\n",
    "    # Training\n",
    "    \"n_epoch\": N_EPOCH,\n",
    "    \"save_ckpt_every_n_epoch\": SAVE_CKPT_EVERY_N_EPOCH,\n",
    "    \"load_ckpt\": LOAD_CKPT,\n",
    "    \"load_ckpt_filepath\": LOAD_CKPT_FILEPATH,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19ce36b3-5ca3-42d7-b68e-7e0ca4e0c1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0c1fb7bcda4a829f9d26b48870a71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='185.275 MB of 417.744 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.44…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▃▅▇█</td></tr><tr><td>train_acc__class_0</td><td>▁▃▅▇█</td></tr><tr><td>train_acc__class_1</td><td>▁▄▅▇█</td></tr><tr><td>train_acc__class_2</td><td>▁▃▅▇█</td></tr><tr><td>train_acc__class_3</td><td>▁▁▂▅█</td></tr><tr><td>train_acc__class_4</td><td>▂▁▃▇█</td></tr><tr><td>train_acc__class_5</td><td>▁▃▄▆█</td></tr><tr><td>train_acc__class_6</td><td>▁▄▆▇█</td></tr><tr><td>train_acc__class_7</td><td>▁▅▆▇█</td></tr><tr><td>train_acc__class_8</td><td>▁▂▅▇█</td></tr><tr><td>train_acc__class_9</td><td>▁▄▆██</td></tr><tr><td>train_loss</td><td>█▆▄▂▁</td></tr><tr><td>train_loss__class_0</td><td>█▆▄▂▁</td></tr><tr><td>train_loss__class_1</td><td>█▅▃▂▁</td></tr><tr><td>train_loss__class_2</td><td>█▆▄▃▁</td></tr><tr><td>train_loss__class_3</td><td>█▆▅▃▁</td></tr><tr><td>train_loss__class_4</td><td>█▇▅▃▁</td></tr><tr><td>train_loss__class_5</td><td>█▆▄▂▁</td></tr><tr><td>train_loss__class_6</td><td>█▆▄▂▁</td></tr><tr><td>train_loss__class_7</td><td>█▆▄▂▁</td></tr><tr><td>train_loss__class_8</td><td>█▆▄▂▁</td></tr><tr><td>train_loss__class_9</td><td>█▆▄▂▁</td></tr><tr><td>valid_acc</td><td>▁▂▄██</td></tr><tr><td>valid_acc__class_0</td><td>▁▄█▅▇</td></tr><tr><td>valid_acc__class_1</td><td>▁▄▄▆█</td></tr><tr><td>valid_acc__class_2</td><td>▁▁▅▅█</td></tr><tr><td>valid_acc__class_3</td><td>▂▂▁▆█</td></tr><tr><td>valid_acc__class_4</td><td>█▅▁▇▅</td></tr><tr><td>valid_acc__class_5</td><td>▁▄▃█▅</td></tr><tr><td>valid_acc__class_6</td><td>▁▃█▆█</td></tr><tr><td>valid_acc__class_7</td><td>▂▆▁▇█</td></tr><tr><td>valid_acc__class_8</td><td>▄▁▅█▆</td></tr><tr><td>valid_acc__class_9</td><td>▁█▆█▁</td></tr><tr><td>valid_loss</td><td>██▇▁▂</td></tr><tr><td>valid_loss__class_0</td><td>█▇▁▅▃</td></tr><tr><td>valid_loss__class_1</td><td>█▅▆▄▁</td></tr><tr><td>valid_loss__class_2</td><td>▅█▄▃▁</td></tr><tr><td>valid_loss__class_3</td><td>▆▅█▁▁</td></tr><tr><td>valid_loss__class_4</td><td>▃▄█▁▂</td></tr><tr><td>valid_loss__class_5</td><td>█▇▇▁▄</td></tr><tr><td>valid_loss__class_6</td><td>▇█▁▅▁</td></tr><tr><td>valid_loss__class_7</td><td>▆▃█▁▂</td></tr><tr><td>valid_loss__class_8</td><td>▅█▄▁▆</td></tr><tr><td>valid_loss__class_9</td><td>▁▂▅▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.52898</td></tr><tr><td>train_acc__class_0</td><td>0.74359</td></tr><tr><td>train_acc__class_1</td><td>0.71162</td></tr><tr><td>train_acc__class_2</td><td>0.36498</td></tr><tr><td>train_acc__class_3</td><td>0.33882</td></tr><tr><td>train_acc__class_4</td><td>0.41451</td></tr><tr><td>train_acc__class_5</td><td>0.39093</td></tr><tr><td>train_acc__class_6</td><td>0.48102</td></tr><tr><td>train_acc__class_7</td><td>0.56038</td></tr><tr><td>train_acc__class_8</td><td>0.62221</td></tr><tr><td>train_acc__class_9</td><td>0.65923</td></tr><tr><td>train_loss</td><td>1.26745</td></tr><tr><td>train_loss__class_0</td><td>0.80027</td></tr><tr><td>train_loss__class_1</td><td>0.87018</td></tr><tr><td>train_loss__class_2</td><td>1.64552</td></tr><tr><td>train_loss__class_3</td><td>1.66252</td></tr><tr><td>train_loss__class_4</td><td>1.57143</td></tr><tr><td>train_loss__class_5</td><td>1.57408</td></tr><tr><td>train_loss__class_6</td><td>1.35384</td></tr><tr><td>train_loss__class_7</td><td>1.21452</td></tr><tr><td>train_loss__class_8</td><td>1.04231</td></tr><tr><td>train_loss__class_9</td><td>0.94616</td></tr><tr><td>valid_acc</td><td>0.6192</td></tr><tr><td>valid_acc__class_0</td><td>0.76</td></tr><tr><td>valid_acc__class_1</td><td>0.971</td></tr><tr><td>valid_acc__class_2</td><td>0.685</td></tr><tr><td>valid_acc__class_3</td><td>0.456</td></tr><tr><td>valid_acc__class_4</td><td>0.415</td></tr><tr><td>valid_acc__class_5</td><td>0.465</td></tr><tr><td>valid_acc__class_6</td><td>0.789</td></tr><tr><td>valid_acc__class_7</td><td>0.631</td></tr><tr><td>valid_acc__class_8</td><td>0.534</td></tr><tr><td>valid_acc__class_9</td><td>0.486</td></tr><tr><td>valid_loss</td><td>1.25961</td></tr><tr><td>valid_loss__class_0</td><td>0.75297</td></tr><tr><td>valid_loss__class_1</td><td>0.07838</td></tr><tr><td>valid_loss__class_2</td><td>0.83158</td></tr><tr><td>valid_loss__class_3</td><td>1.46146</td></tr><tr><td>valid_loss__class_4</td><td>1.57044</td></tr><tr><td>valid_loss__class_5</td><td>1.43248</td></tr><tr><td>valid_loss__class_6</td><td>0.68566</td></tr><tr><td>valid_loss__class_7</td><td>1.34215</td></tr><tr><td>valid_loss__class_8</td><td>1.93279</td></tr><tr><td>valid_loss__class_9</td><td>2.50819</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">woven-plant-48</strong>: <a href=\"https://wandb.ai/brianryan/pure-noise/runs/1tk5q7b2\" target=\"_blank\">https://wandb.ai/brianryan/pure-noise/runs/1tk5q7b2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221226_150146-1tk5q7b2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "for epoch_i in range(N_EPOCH):\n",
    "    # Save checkpoint\n",
    "    if epoch_i % SAVE_CKPT_EVERY_N_EPOCH == 0:\n",
    "        checkpoint_filepath = f\"checkpoints/{wandb.run.name}__epoch_{epoch_i}.pt\"\n",
    "        os.makedirs(\"checkpoints/\", exist_ok=True)\n",
    "        save_checkpoint(net, optimizer, checkpoint_filepath)\n",
    "        wandb.save(checkpoint_filepath)\n",
    "\n",
    "    ## Training Phase\n",
    "    net.train()\n",
    "    train_losses = []\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "    for minibatch_i, (inputs, labels) in enumerate(train_loader):\n",
    "        if epoch_i >= OPEN__START_EPOCH:\n",
    "            inputs, noise_mask = replace_images_with_pure_noise(\n",
    "                inputs,\n",
    "                labels,\n",
    "                mean_per_channel,\n",
    "                std_per_channel,\n",
    "                height=DATASET__IMAGE_HEIGHT,\n",
    "                width=DATASET__IMAGE_WIDTH,\n",
    "                noise_ratio=OPEN__NOISE_RATIO,\n",
    "            )\n",
    "\n",
    "        inputs = inputs.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        losses = criterion(outputs, labels)\n",
    "        losses.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_losses.extend(losses.cpu().detach().tolist())\n",
    "        train_labels.extend(labels.cpu().detach().tolist())\n",
    "        train_preds.extend(preds.cpu().detach().tolist())\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_preds = np.array(train_preds)\n",
    "\n",
    "    # Filter losses by classes\n",
    "    train_loss_per_class_dict = {\n",
    "        f\"train_loss__class_{class_}\": train_losses[np.where(train_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(10)\n",
    "    }\n",
    "    # Filter preds by classes for accuracy\n",
    "    train_acc_per_class_dict = {\n",
    "        f\"train_acc__class_{class_}\": (train_preds == train_labels)[np.where(train_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(10)\n",
    "    }\n",
    "\n",
    "    ## Validation Phase\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # Save all losses and labels for each example\n",
    "        valid_losses = []\n",
    "        valid_labels = []\n",
    "        valid_preds = []\n",
    "        for minibatch_i, (inputs, labels) in enumerate(valid_loader):\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            losses = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            valid_losses.extend(losses.cpu().detach().tolist())\n",
    "            valid_labels.extend(labels.cpu().detach().tolist())\n",
    "            valid_preds.extend(preds.cpu().detach().tolist())\n",
    "\n",
    "    valid_losses = np.array(valid_losses)\n",
    "    valid_labels = np.array(valid_labels)\n",
    "    valid_preds = np.array(valid_preds)\n",
    "\n",
    "    # Filter losses by classes\n",
    "    valid_loss_per_class_dict = {\n",
    "        f\"valid_loss__class_{class_}\": valid_losses[np.where(valid_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(10)\n",
    "    }\n",
    "    # Filter preds by classes for accuracy\n",
    "    valid_acc_per_class_dict = {\n",
    "        f\"valid_acc__class_{class_}\": (valid_preds == valid_labels)[np.where(valid_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(10)\n",
    "    }\n",
    "\n",
    "    # Logging\n",
    "    wandb.log({\n",
    "        \"train_loss\": np.mean(train_losses),\n",
    "        \"train_acc\": np.mean(train_preds == train_labels),\n",
    "        **train_loss_per_class_dict,\n",
    "        **train_acc_per_class_dict,\n",
    "        \"valid_loss\": np.mean(valid_losses),\n",
    "        \"valid_acc\": np.mean(valid_preds == valid_labels),\n",
    "        **valid_loss_per_class_dict,\n",
    "        **valid_acc_per_class_dict,\n",
    "    })\n",
    "    if epoch_i in [160, 180]:\n",
    "        scheduler.step()\n",
    "\n",
    "# Save final checkpoint\n",
    "checkpoint_filepath = f\"checkpoints/{wandb.run.name}__epoch_{N_EPOCH}.pt\"\n",
    "os.makedirs(\"checkpoints/\", exist_ok=True)\n",
    "save_checkpoint(net, optimizer, checkpoint_filepath)\n",
    "wandb.save(checkpoint_filepath)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439fad92-ac39-4eb6-8ee4-d448b583a5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
