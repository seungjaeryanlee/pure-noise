{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db3a3c8-f8a6-46f8-9a97-22fd99ba0baa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917ed30-62af-43a3-bf14-87272091f85b",
   "metadata": {},
   "source": [
    "Description from Section D.2:\n",
    "\n",
    "![Description from Section D.2](images/paper__image_augmentations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9734eff9-24c0-46f4-8909-81036431a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import custom_transforms\n",
    "\n",
    "train_dataset_mean = (0.5039, 0.4333, 0.3989)\n",
    "train_dataset_std = (0.3200, 0.3014, 0.3016)\n",
    "\n",
    "# transforms.ToTensor() not needed as we use torchvision.io.read_image,\n",
    "# which gives torch.Tensor instead of PIL.Image\n",
    "# Data Augmentation transforms are mostly from Bazinga699/NCL\n",
    "# https://github.com/Bazinga699/NCL/blob/2bbf193/lib/dataset/cui_cifar.py#L64\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(64, padding=4),\n",
    "    custom_transforms.Cutout(n_holes=1, length=16),\n",
    "    # TODO: Check if this is correct values for SIMCLR augmentation\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=[.1, 2.])\n",
    "    ], p=0.5),\n",
    "    # TODO: Verify where this number comes from: is it CIFAR-10 or CIFAR-10-LT?\n",
    "    transforms.Normalize(train_dataset_mean, train_dataset_std),\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize(train_dataset_mean, train_dataset_std),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize(train_dataset_mean, train_dataset_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57e2e98-eec3-4aba-8efb-1fd5d4728552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.celeba5 import CelebA5Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a7bbfa-b4be-4a08-973c-9794c9e20226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/CelebA5/train\"\n",
    "train_dataset = CelebA5Dataset(\n",
    "    dataset_path=train_dir,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac486f42-2b29-4d45-a598-385042e0e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir = \"data/CelebA5/valid\"\n",
    "valid_dataset = CelebA5Dataset(\n",
    "    dataset_path=valid_dir,\n",
    "    transform=valid_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf8745f-6316-4f38-84c1-18581caeb2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6651, 250)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8acab3-8de7-4930-b879-9b2208fa3596",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139825f5-9b45-4cc7-804e-d864ce2c1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader Hyperparameters\n",
    "DATALOADER__NUM_WORKERS = 0\n",
    "DATALOADER__BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210e0e53-2c2a-40c2-bbd0-886616b0df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sample_labels = []\n",
    "sample_labels_count = np.arange(5)\n",
    "with open(os.path.join(train_dir, 'labels.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        _, label = line.split()\n",
    "        label = int(label)\n",
    "        sample_labels.append(label)\n",
    "        sample_labels_count[label] += 1\n",
    "weights = 1. / sample_labels_count\n",
    "sample_weights = np.array([weights[l] for l in sample_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bccb704e-4d0e-4ac9-94e3-9db48d9a8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=6651, # https://stackoverflow.com/a/67802529\n",
    "    replacement=True,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=DATALOADER__BATCH_SIZE,\n",
    "    num_workers=DATALOADER__NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19d3c2b-d8ef-4fc5-9c08-74d9babe7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=DATALOADER__BATCH_SIZE,\n",
    "    num_workers=DATALOADER__NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c0688-22cf-4900-acc3-c54608332a57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09745284-43dc-47f5-9ef9-9bfd479de060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "MODEL__WIDERESNET_DEPTH = 28\n",
    "MODEL__WIDERESNET_K = 10\n",
    "MODEL__WIDERESNET_DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e17c2bf-246f-4846-80f3-8107b001484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks_torchdistill import WideBasicBlock, WideResNet\n",
    "\n",
    "net = WideResNet(\n",
    "    depth=MODEL__WIDERESNET_DEPTH,\n",
    "    k=MODEL__WIDERESNET_K,\n",
    "    dropout_p=MODEL__WIDERESNET_DROPOUT,\n",
    "    block=WideBasicBlock,\n",
    "    num_classes=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e331334-268a-4fc5-8d6a-21e45b0896f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networks import WideResNet\n",
    "\n",
    "# # TODO: Consider replacing with https://github.com/yoshitomo-matsubara/torchdistill/blob/main/torchdistill/models/classification/wide_resnet.py\n",
    "# net = WideResNet(\n",
    "#     num_classes=10,\n",
    "#     depth=MODEL__WIDERESNET_DEPTH,\n",
    "#     widen_factor=MODEL__WIDERESNET_K,\n",
    "#     dropRate=MODEL__WIDERESNET_DROPOUT,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33260cab-02b3-4850-8c4a-e35e80f8e4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36475989"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b4f9a70-ae5a-410d-8640-aa673e981ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cbd19-4d6b-442c-ad4f-d731160cdc41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8025d5c2-6781-42f0-bfb5-5254a9414b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbriansmlee\u001b[0m (\u001b[33mbrianryan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a69ed-fd35-4c03-9519-0095f4315891",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a90c9d5f-e0f1-4765-8c3b-f6d16f23d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Hyperparameters\n",
    "OPTIM__LR = 0.1\n",
    "OPTIM__MOMENTUM = 0.9\n",
    "OPTIM__WEIGHT_DECAY = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e56120d-408a-432d-9eff-ddfd0e37af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=OPTIM__LR,\n",
    "    momentum=OPTIM__MOMENTUM,\n",
    "    weight_decay=OPTIM__WEIGHT_DECAY,\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1,\n",
    "    gamma=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fbe8e-b9cd-4287-bcbf-2d40b3081acb",
   "metadata": {},
   "source": [
    "## Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208cf7e0-df2f-45a3-849c-49cc542d4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "N_EPOCH = 90\n",
    "SAVE_CKPT_EVERY_N_EPOCH = 10\n",
    "LOAD_CKPT = False\n",
    "LOAD_CKPT_FILEPATH = \"checkpoints/.pt\"\n",
    "LOAD_CKPT_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451ccd42-20a2-4679-9ef3-9e3f26e72a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3282946-999f-4ff3-8786-2b17f84eeb31",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "046cc698-7fef-486e-b705-da632b963f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    checkpoint_filepath: str,\n",
    "):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    checkpoint_filepath: str,\n",
    "):\n",
    "    checkpoint = torch.load(checkpoint_filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bf0b5f1-8d8f-45ff-bc2e-7feb04b9aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CKPT:\n",
    "    load_checkpoint(net, optimizer, LOAD_CKPT_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51ab08a5-1503-4c02-bf5c-4104a8d720dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:emwm81mf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559b9d1be1c646e49060a40f0be72d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='139.274 MB of 139.274 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fanciful-pyramid-81</strong>: <a href=\"https://wandb.ai/brianryan/pure-noise/runs/emwm81mf\" target=\"_blank\">https://wandb.ai/brianryan/pure-noise/runs/emwm81mf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230102_131818-emwm81mf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:emwm81mf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d90a19e6575485395755298a36b7db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666995391715318, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/pure-noise/wandb/run-20230102_131913-1v4f67ck</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/brianryan/pure-noise/runs/1v4f67ck\" target=\"_blank\">ethereal-sun-82</a></strong> to <a href=\"https://wandb.ai/brianryan/pure-noise\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"pure-noise\",\n",
    "    entity=\"brianryan\",\n",
    ")\n",
    "\n",
    "wandb.config.update({\n",
    "    # Data\n",
    "    \"dataloader__num_workers\": DATALOADER__NUM_WORKERS,\n",
    "    \"dataloader__batch_size\": DATALOADER__BATCH_SIZE,\n",
    "    # Optimizer\n",
    "    \"optim__lr\": OPTIM__LR,\n",
    "    \"optim__momentum\": OPTIM__MOMENTUM,\n",
    "    \"optim__weight_decay\": OPTIM__WEIGHT_DECAY,\n",
    "    # Model\n",
    "    \"model__wideresnet_depth\": MODEL__WIDERESNET_DEPTH,\n",
    "    \"model__wideresnet_k\": MODEL__WIDERESNET_K,\n",
    "    \"model__wideresnet_dropout\": MODEL__WIDERESNET_DROPOUT,\n",
    "    # Training\n",
    "    \"n_epoch\": N_EPOCH,\n",
    "    \"save_ckpt_every_n_epoch\": SAVE_CKPT_EVERY_N_EPOCH,\n",
    "    \"load_ckpt\": LOAD_CKPT,\n",
    "    \"load_ckpt_filepath\": LOAD_CKPT_FILEPATH,\n",
    "    \"load_ckpt_epoch\": LOAD_CKPT_EPOCH,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce36b3-5ca3-42d7-b68e-7e0ca4e0c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "start_epoch_i, end_epoch_i = 0, N_EPOCH\n",
    "if LOAD_CKPT:\n",
    "    start_epoch_i += LOAD_CKPT_EPOCH\n",
    "    end_epoch_i += LOAD_CKPT_EPOCH\n",
    "for epoch_i in range(start_epoch_i, end_epoch_i):\n",
    "    # Save checkpoint\n",
    "    if epoch_i % SAVE_CKPT_EVERY_N_EPOCH == 0:\n",
    "        checkpoint_filepath = f\"checkpoints/{wandb.run.name}__epoch_{epoch_i}.pt\"\n",
    "        os.makedirs(\"checkpoints/\", exist_ok=True)\n",
    "        save_checkpoint(net, optimizer, checkpoint_filepath)\n",
    "        wandb.save(checkpoint_filepath)\n",
    "\n",
    "    ## Training Phase\n",
    "    net.train()\n",
    "    train_losses = []\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "    for minibatch_i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        losses = criterion(outputs, labels)\n",
    "        losses.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_losses.extend(losses.cpu().detach().tolist())\n",
    "        train_labels.extend(labels.cpu().detach().tolist())\n",
    "        train_preds.extend(preds.cpu().detach().tolist())\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_preds = np.array(train_preds)\n",
    "\n",
    "    # Filter losses by classes\n",
    "    train_loss_per_class_dict = {\n",
    "        f\"train_loss__class_{class_}\": train_losses[np.where(train_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(5)\n",
    "    }\n",
    "    # Filter preds by classes for accuracy\n",
    "    train_acc_per_class_dict = {\n",
    "        f\"train_acc__class_{class_}\": (train_preds == train_labels)[np.where(train_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(5)\n",
    "    }\n",
    "\n",
    "    ## Validation Phase\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # Save all losses and labels for each example\n",
    "        valid_losses = []\n",
    "        valid_labels = []\n",
    "        valid_preds = []\n",
    "        for minibatch_i, (inputs, labels) in enumerate(valid_loader):\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            losses = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            valid_losses.extend(losses.cpu().detach().tolist())\n",
    "            valid_labels.extend(labels.cpu().detach().tolist())\n",
    "            valid_preds.extend(preds.cpu().detach().tolist())\n",
    "\n",
    "    valid_losses = np.array(valid_losses)\n",
    "    valid_labels = np.array(valid_labels)\n",
    "    valid_preds = np.array(valid_preds)\n",
    "\n",
    "    # Filter losses by classes\n",
    "    valid_loss_per_class_dict = {\n",
    "        f\"valid_loss__class_{class_}\": valid_losses[np.where(valid_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(5)\n",
    "    }\n",
    "    # Filter preds by classes for accuracy\n",
    "    valid_acc_per_class_dict = {\n",
    "        f\"valid_acc__class_{class_}\": (valid_preds == valid_labels)[np.where(valid_labels == class_)[0]].mean()\n",
    "        for class_ in np.arange(5)\n",
    "    }\n",
    "\n",
    "    # Logging\n",
    "    wandb.log({\n",
    "        \"epoch_i\": epoch_i,\n",
    "        \"train_loss\": np.mean(train_losses),\n",
    "        \"train_acc\": np.mean(train_preds == train_labels),\n",
    "        **train_loss_per_class_dict,\n",
    "        **train_acc_per_class_dict,\n",
    "        \"valid_loss\": np.mean(valid_losses),\n",
    "        \"valid_acc\": np.mean(valid_preds == valid_labels),\n",
    "        **valid_loss_per_class_dict,\n",
    "        **valid_acc_per_class_dict,\n",
    "    })\n",
    "    if epoch_i in [30, 60]:\n",
    "        scheduler.step()\n",
    "\n",
    "# Finish wandb run\n",
    "wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d388e7d-350e-4c8d-a97c-834baa2469a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
